{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">–î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ1 </span>\n",
    "\n",
    "<span style=\"color: red; font-size: 14pt\">Deadline: 27.02.2018 23:59:59</span>\n",
    "\n",
    "<span style=\"font-size: 10pt\">–§–ò–í–¢, –ê–ü–¢, –ö—É—Ä—Å –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –í–µ—Å–Ω–∞ 2018</span>\n",
    "\n",
    "<span style=\"color:blue; font-size: 10pt\">Alexey Romanenko, </span>\n",
    "<span style=\"color:blue; font-size: 10pt; font-family: 'Verdana'\">alexromsput@gmail.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∑**:\n",
    "- –í–æ—Ä–æ–Ω—Ü–æ–≤ –ö. –í. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –ø–æ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–∞–º. 2012. \n",
    "- –ú–µ—Ä–∫–æ–≤ –ê. –ë. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–µ—Ç–æ–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ï–¥–∏—Ç–æ—Ä–∏–∞–ª –£–†–°–°. 2011. 256 —Å—Ç—Ä.\n",
    "- Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. Springer: Data Mining, Inference, and Prediction. ‚Äî 2nd ed. ‚Äî Springer-Verlag. 2009. ‚Äî 746 p.\n",
    "- C. M. Bishop. Pattern Recognition and Machine Learning. ‚Äî Springer, Series: Information Science and Statistics. 2006. ‚Äî 738 p.\n",
    "\n",
    "**–û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –¥–∑**: \n",
    "- –ü—Ä–∏—Å—ã–ª–∞–π—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –ø–æ—á—Ç—É ``ml.course.mipt@gmail.com``\n",
    "- –£–∫–∞–∂–∏—Ç–µ —Ç–µ–º—É –ø–∏—Å—å–º–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ ``ML2018_fall <–Ω–æ–º–µ—Ä_–≥—Ä—É–ø–ø—ã> <—Ñ–∞–º–∏–ª–∏—è>``, –∫ –ø—Ä–∏–º–µ—Ä—É -- ``ML2018_fall 596 ivanov``\n",
    "- –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –¥–∑ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ —Ñ–∞–π–ª ``ML2018_<—Ñ–∞–º–∏–ª–∏—è>_<–≥—Ä—É–ø–ø–∞>_task<–Ω–æ–º–µ—Ä –∑–∞–¥–∞–Ω–∏—è>.ipnb``, –∫ –ø—Ä–∏–º–µ—Ä—É -- ``ML2018_ivanov_596_task1.ipnb``\n",
    "\n",
    "**–í–æ–ø—Ä–æ—Å—ã**:\n",
    "- –ü—Ä–∏—Å—ã–ª–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ—á—Ç—É ``ml.course.mipt@gmail.com``\n",
    "- –£–∫–∞–∂–∏—Ç–µ —Ç–µ–º—É –ø–∏—Å—å–º–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ ``ML2018_fall Question <–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞>``\n",
    "\n",
    "--------\n",
    "- **PS1**: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã, –∏ –ø—Ä–æ—Å—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–º –≤–∞—à–µ –¥–∑, –µ—Å–ª–∏ –≤—ã –Ω–µ–∞–∫–∫—É—Ä–∞—Ç–Ω–æ –µ–≥–æ –ø–æ–¥–ø–∏—à–∏—Ç–µ.\n",
    "- **PS2**: –ü—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã–π –¥–µ–¥–ª–∞–π–Ω —Å–Ω–∏–∂–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –∑–∞–¥–∞–Ω–∏—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ, —É–∫–∞–∑–Ω–Ω–æ–π –Ω–∞ –ø–µ—Ä–≤–æ–º —Å–µ–º–∏–Ω–∞—Ä–µ\n",
    "- **PS3**: –î–æ–ø—É—Å—Ç–∏–º—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –Ω–∏–∂–µ, –µ—Å–ª–∏ –≤—ã —Å—á–∏—Ç–∞–µ—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è </h1> \n",
    "\n",
    "–ù–∏–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞–Ω–∏—è.\n",
    "\n",
    "* –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è\n",
    "    1. –ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏? –ö–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ supervised learning, –∞ –∫–∞–∫–∏–µ - –∫ unsupervised learning?\n",
    "    2. –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ? –ö–∞–∫ –∏—Ö –º–æ–∂–Ω–æ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å?\n",
    "    3. –ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—É—á–∞—é—â–∞—è –∏ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∏, –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è? –ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ k-fold cross validation?\n",
    "    4. –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –º–µ—Ç–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ bias –∏ Variance: –∫–∞–∫ –≤–µ–¥—É—Ç —Å–µ–±—è –æ–±–µ –≤–µ–ª–∏—á–∏–Ω—ã, –∫–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–µ–º–µ–π—Å—Ç–≤–∞ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤?\n",
    "\n",
    "\n",
    "* –ü—Ä–æ—Å—Ç—ã–µ –º–µ—Ç–æ–¥—ã\n",
    "    1. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç kNN –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏?\n",
    "    2. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç kNN —Å –≤–µ—Å–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –≤ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?\n",
    "    3. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –≤ —á–µ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –µ–≥–æ \"–Ω–∞–∏–≤–Ω–æ—Å—Ç—å\"?\n",
    "    4. –ö–∞–∫ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å y –æ—Ç x –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –≤–µ—Å–∞ –≤ –Ω–µ–π?\n",
    "    5. –í —á–µ–º —Å—É—Ç—å –ø—Ä–æ–∫–ª—è—Ç–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏?\n",
    "\n",
    "\n",
    "* –ò–∑–º–µ—Ä–µ–Ω–∏–µ –æ—à–∏–±–∫–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "    1. –ö–∞–∫ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∏ –≤ –∫–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏/—Ä–µ–≥—Ä–µ—Å—Å–∏–∏) –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏:\n",
    "    accuracy, precision, recall, F1-measure, ROC-AUC, PR-AUC, MSE, MAE, RMSE?\n",
    "    2. –†–µ—à–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ - 0 –∏ 1), –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏-\n",
    "    –º–µ—Ä—ã –∏–∑ –∫–ª–∞—Å—Å–∞ 0 —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç 95% –≤—ã–±–æ—Ä–∫–∏. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤ –ø—Ä–µ–¥—ã-\n",
    "    –¥—É—â–µ–º –≤–æ–ø—Ä–æ—Å–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?\n",
    "    3. –ö –æ—Ü–µ–Ω–∫–µ –∫–∞–∫–æ–π –≤–µ–ª–∏—á–∏–Ω—ã –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è y –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ x –ø—Ä–∏–≤–æ–¥—è—Ç MSE –∏ MAE?\n",
    "    4. –ö–∞–∫–∞—è –∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–æ–ª–∂–Ω—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª —Ä–∞–≤–µ–Ω –º–µ–¥–∏–∞–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–π —Å–ª—É—á–∞–π–Ω–æ –≤–µ–ª–∏—á–∏–Ω—ã?\n",
    "\n",
    "\n",
    "* Python, numpy, scipy, matplotlib, sklearn, pandas\n",
    "    1. –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö list, tuple, dict, set, str, unicode, hashable –∏ unhashable. \n",
    "    2. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã numpy –∏ scipy? –ö–∞–∫–æ–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ numpy –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å\n",
    "    –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏? –û—Ç–ª–∏—á–∏—è –≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ ndarray –∏ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤.\n",
    "    3. –ö–∞–∫ –≤ scipy —Ä–µ—à–∏—Ç—å —á–∏—Å–ª–µ–Ω–Ω–æ –Ω–µ—Å–ª–æ–∂–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É? –ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–ø-\n",
    "    —Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã?\n",
    "    4. –ö–∞–∫ –ø–æ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π x –∏ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π y –≤ —ç—Ç–∏—Ö —Ç–æ—á–∫–∞—Ö –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ y(x) –≤\n",
    "    matplotlib?\n",
    "    5. –ö–∞–∫ –≤ sklearn –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π?\n",
    "    6. –ö–∞–∫–∏–µ –µ—Å—Ç—å —Å—Ä–µ–¥—Å—Ç–≤–∞ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤ sklearn? –ö–∞–∫ –ø–æ—Å—á—Ç–∏—Ç–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "    –≤ k-fold cross validation?\n",
    "    7. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ cross_val_score –∏–∑ sklearn?\n",
    "    8. –ö–∞–∫ —Å—á–∏—Ç–∞—Ç—å –≤—ã–±–æ—Ä–∫—É –∏–∑ csv –≤ pandas DataFrame? –ê –∫–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å DataFrame –≤ —Ñ–∞–π–ª? –ö–∞–∫\n",
    "    —É–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/–∑–∞–ø–∏—Å–∏ –∫–æ–¥–∏—Ä–æ–≤–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, –Ω–∞–ª–∏—á–∏–µ/–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ\n",
    "    –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —É –∫–æ–ª–æ–Ω–æ–∫?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (2 –±–∞–ª–ª–∞) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** –ó–∞–¥–∞—á–∞ 1**\n",
    "–ü–æ–∫–∞–∂–∏—Ç–µ, —á—Ç–æ ROC-AUC  –≤ —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–∞—ë—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã $a(x) = 1$ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $p$ –∏ $a(x) = 0$ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $1 ‚àí p$, –±—É–¥–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º —Ä–∞–≤–µ–Ω 0.5 –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç p –∏ –¥–æ–ª–∏ –∫–ª–∞—Å—Å–∞ 1 –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "\n",
    "<–†–µ—à–µ–Ω–∏–µ:>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** –ó–∞–¥–∞—á–∞ 2**\n",
    "–ü–æ–∫–∞–∂–∏—Ç–µ, —á—Ç–æ —Å —Ä–æ—Å—Ç–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–æ—á–µ–∫ –≤ –∫—É–±–µ $[0; 1]^ùëë$ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ø–∞—Å—Ç—å –≤ –∫—É–± $[0; 0, 99]^ùëë$ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –Ω—É–ª—é. –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π –ø—Ä–æ–∫–ª—è—Ç–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (dimension curse). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å –∏–ª–∏ –Ω–∞–π—Ç–∏ –µ—â–µ –∫–∞–∫—É—é-–Ω–∏–±—É–¥—å –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—é –∫ —ç—Ç–æ–º—É —è–≤–ª–µ–Ω–∏—é –∏ –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏—Ç—å.\n",
    "\n",
    "<–†–µ—à–µ–Ω–∏–µ:>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">–ò—Å—Å–ª–µ–¥—É–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å Bias-Variance –¥–ª—è kNN (3 –±–∞–ª–ª–∞) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ—Ö–æ–¥–æ–≤ –∏–Ω–¥–∏–≤–∏–¥—É—É–º–∞ http://archive.ics.uci.edu/ml/machine-learning-databases/adult, –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ–ª–∏—á–∏–Ω bias, variance –∏ noise –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞—Ä \"–º–æ–¥–µ–ª—å|–ø–∞—Ä–∞–º–µ—Ç—Ä\".\n",
    "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ä–∞–∑–º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–æ–≤) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤–∞–º–∏. \n",
    "\n",
    "* kNN, $n\\_neigbours$ - —á–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏;\n",
    "* $p$ - —Å—Ç–µ–ø–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ –ú–∏–Ω–∫–æ–≤—Å–∫–æ–≥–æ;\n",
    "* $\\ell\\_train$ - –¥–ª–∏–Ω–∞ –æ–±—É—á–∞—é—â–∏–π –≤—ã–±–æ—Ä–∫–∏;\n",
    "\n",
    "\n",
    "–ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ–∏–∫—É (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫) –æ–±—ä—è—Å–Ω–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É. –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –æ–Ω–∞ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ (—Å–º. —Å–µ–º–∏–Ω–∞—Ä 2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "\n",
    "# Settings\n",
    "n_repeat = 100       # Number of iterations for computing expectations\n",
    "test_size = 0.5       # (Relative) Size of the test set\n",
    "\n",
    "# –ò—Å—Å–ª–µ–¥—É–µ–º–∞—è —Å–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "# –ø–æ—è—Å–Ω–µ–Ω–∏–µ: –¥–ª–∏–Ω–∞ train –≤—ã–±–æ—Ä–∫–∏ –¥–æ–ª–∂–Ω–∞ –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –æ—Ç 50% –¥–æ 200% –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–ª–∏–Ω—ã —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏  \n",
    "parameters = {\"k\": range(20), \"p\":  np.arange(1, 10, 0.5), \"l_train\": np.arange(0.5, 2, 0.1)}\n",
    "\n",
    "# Get Data\n",
    "train, test = —Å–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–±–µ–π—Ç–µ –∏—Ö –Ω–∞ —Ç–µ—Å—Ç –∏ —Ç—Ä–µ–π–Ω\n",
    "\n",
    "\n",
    "# –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å bias –∏ variance, –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º \n",
    "#–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ –¥–∞–Ω–Ω—ã—Ö \n",
    "# (–±—É—Ç—Å—Ç—Ä–µ–ø–∏–Ω–≥)\n",
    "for i in range(n_repeat):\n",
    "    sample_train = train.sample(bootstrap_size=len(train), replace=True) \n",
    "    X, y = —Ä–∞–∑–±–µ–π—Ç–µ sample_train –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4d9f9de6328b>, line 11)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4d9f9de6328b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    KNeighborsClassifier(–ø–∞—Ä–∞–º–µ—Ç—Ä = –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞).fit(X_train[i], y_train[i])\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ä–∞ –æ—Ü–µ–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ error, bias –∏ variance —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–µ\n",
    "p_name ='p'\n",
    "bias_variance_df = pd.DataFrame.from_dict({p_name:parameters[p_name],'bias':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'variance':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'error':list([np.NaN]*len(parameters[p_name]))})\n",
    "for parameter in parameters[p_name]:\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        KNeighborsClassifier(–ø–∞—Ä–∞–º–µ—Ç—Ä = –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞).fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "    # –í –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ bias –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º –∫–≤–∞–¥—Ä–∞—Ç—É —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "    # –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º.\n",
    "    y_bias = (y_test - np.mean(y_predict, axis=1)) ** 2\n",
    "    \n",
    "    # Variance –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É —Ä–∞–∑–±—Ä–æ—Å—É –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º. \n",
    "    # –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ bias –∏ variance –≤–æ–∑—å–º—ë–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–º—É –ø–æ –≤—Å–µ–º —Ç–æ—á–∫–∞–º —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "    \n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'bias'] = —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ bias^2 –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'variance'] = —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ variance –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'error'] = —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏ error –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "    \n",
    "# –Ω–∞—Ä–∏—Å—É–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å bias, variance –∏ error –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "plt.figure(figsize=(10, 8))\n",
    "df.set_index(p_name).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">–†–µ–∞–ª–∏–∑—É–π—Ç–µ kNN (2 –±–∞–ª–ª–∞)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class kNNClassifier():\n",
    "    def __init__(self, n_estimators, metric):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_neighbours: int\n",
    "            –ß–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π\n",
    "\n",
    "        metric: *alias\n",
    "            –º–µ—Ç—Ä–∏–∫–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π\n",
    "\n",
    "          \"\"\"\n",
    "        self.n_neighbours = base_estimator\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "\n",
    "        # –¢—É—Ç —Ö—Ä–∞–Ω–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        self.X_learn = ...\n",
    "\n",
    "        # –¢—É—Ç —Ö—Ä–∞–Ω–∏—Ç–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É –æ–±—ä–µ–∫—Ç—É –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        self.y_learn = ...\n",
    "\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä–µ–∫—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –æ—Ç–≤–µ—Ç\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, –≤–µ–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞\n",
    "        \"\"\"\n",
    "        \n",
    "        dist = [] # –•—Ä–∞–Ω–∏—Ç–µ —Ç—É—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ \n",
    "        \n",
    "        for i in range(self.X_learn):\n",
    "            # =======================================\n",
    "            # —Ä–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            # ======================================\n",
    "            ...\n",
    "\n",
    "        # =======================================\n",
    "        # –ø—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ –∫–ª–∞—Å—Å –∫–∞–∂–¥–æ–≥–æ –∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "        # =======================================\n",
    "        y_pred = ...\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–∞—à –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö http://archive.ics.uci.edu/ml/machine-learning-databases/adult\n",
    "from sklearn.metrics import accuracy_score\n",
    "# =======================================\n",
    "# –û–±—É—á–∏—Ç–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏ k=3, 5, –∏ 10\n",
    "# =======================================\n",
    "clf = ...\n",
    "print (accuracy_score(clf.predict(X_train[0]), y_train[0]), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (3 –±–∞–ª–ª–∞) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ADD_DELL –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –µ–≥–æ –¥–ª—è kNN –Ω–∞ –¥–∞–Ω–Ω—ã—Ö sklearn.datasets.digits(). \n",
    "–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏\n",
    "def update_Q_min_index(estimator, Q_min, Q_min_set, Q_min_index, feature_set, X_train, Y_train, X_test, Y_test, update_equal=False):\n",
    "    feature_set_size = len(feature_set)\n",
    "    estimator.fit(X_train[:, feature_set], Y_train)\n",
    "    error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "    if error < Q_min[feature_set_size]:\n",
    "        Q_min_set[feature_set_size] = feature_set\n",
    "        Q_min[feature_set_size] = error\n",
    "        if ((Q_min[feature_set_size] < Q_min[Q_min_index])\n",
    "                or (update_equal and Q_min[feature_set_size] == Q_min[Q_min_index])):\n",
    "            Q_min_index = feature_set_size\n",
    "    return Q_min_index\n",
    "\n",
    "\n",
    "# –ù—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n",
    "def add_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10, start_feature_set=[]):\n",
    "    Q_min = {0: float('+inf')}\n",
    "    Q_min_set = {0: []}\n",
    "    Q_min_index = 0\n",
    "    \n",
    "    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤     \n",
    "    if start_feature_set:\n",
    "        estimator.fit(X_train[:, feature_set], Y_train)\n",
    "        error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "        Q_min = {len(start_feature_set) : error}\n",
    "        Q_min_set = {len(start_feature_set) : start_feature_set}\n",
    "        Q_min_index = len(start_feature_set)\n",
    "    \n",
    "    # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤     \n",
    "    for feature_set_size in range(len(start_feature_set) + 1, len(feature_set) + 1):\n",
    "        Q_min[feature_set_size] = float('+inf')\n",
    "        unused_features = set(feature_set).difference(set(Q_min_set[feature_set_size - 1]))\n",
    "        unused_features = list(unused_features)\n",
    "        #shuffle(unused_features)\n",
    "        for feature in unused_features:\n",
    "            new_feature_set = –æ–±–Ω–æ–≤–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            Q_min_index = –æ–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        \n",
    "        print ('Q_min: %.4lf, set_size: %d, added: %s' % (Q_min[feature_set_size], feature_set_size, Q_min_set[feature_set_size][-1]))        \n",
    "    \n",
    "    # –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞\n",
    "        if Q_min_index + look_forward <= feature_set_size:\n",
    "            break\n",
    "    return Q_min_set[Q_min_index]\n",
    "\n",
    "def del_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10):\n",
    "    \n",
    "#     –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    estimator.fit(X_train[:, feature_set], Y_train)\n",
    "    error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "    Q_min = {len(feature_set): error}\n",
    "    Q_min_set = {len(feature_set): deepcopy(feature_set)}\n",
    "    Q_min_index = len(feature_set)\n",
    "    \n",
    "#     –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    for feature_set_size in list(range(len(feature_set) - 1, 0, -1)):\n",
    "        Q_min[feature_set_size] = float('+inf')\n",
    "        features = copy(Q_min_set[feature_set_size + 1])\n",
    "        # shuffle(features)\n",
    "        for feature in features:\n",
    "            new_feature_set = –æ–±–Ω–æ–≤–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            Q_min_index = –æ–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "            \n",
    "        print ('Q_min: %.4lf, set_size: %d, deleted: %s' % (Q_min[feature_set_size], feature_set_size,\n",
    "            set(Q_min_set[feature_set_size + 1]).difference(set(Q_min_set[feature_set_size])),\n",
    "        ))\n",
    "        \n",
    "#         –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞\n",
    "        if feature_set_size + look_forward <= Q_min_index:\n",
    "            break\n",
    "    return Q_min_set[Q_min_index]\n",
    "\n",
    "# –ù–∞–∫–æ–Ω–µ—Ü, —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ ADD-DELL\n",
    "def add_del(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10):\n",
    "    Q_min = None\n",
    "    Q_min_current = float('+inf')\n",
    "    start_feature_set = []\n",
    "    \n",
    "#     –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π, –ø—Ä–∏–≤–µ–¥—à–∏—Ö –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –æ—à–∏–±–∫–∏\n",
    "    bad_iteration_count = 2\n",
    "    \n",
    "    while (Q_min is None) or (Q_min_current < Q_min or bad_iteration_count):\n",
    "        Q_min = Q_min_current\n",
    "        start_feature_set = add_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward, start_feature_set=start_feature_set)\n",
    "        start_feature_set = del_one(estimator, start_feature_set, X_train, Y_train, X_test, Y_test, look_forward)\n",
    "        estimator.fit(X_train[:, start_feature_set], Y_train)\n",
    "        Q_min_current = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, start_feature_set]))\n",
    "        \n",
    "        if Q_min_current > Q_min:\n",
    "            bad_iteration_count -= 1\n",
    "        print ('Q_min: %.4lf, set_size: %d' % (Q_min_current, len(start_feature_set)))\n",
    "    return start_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = —Å–∫–∞–π—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–±–µ–π—Ç–µ –∏—Ö –Ω–∞ —Ç–µ—Å—Ç/—Ç—Ä–µ–π–Ω\n",
    "\n",
    "# –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é\n",
    "estimator = KNeighborsClassifier()\n",
    "%time feature_set = add_del(estimator, list(range(X_train.shape[1])), X_train, Y_train, X_test, Y_test, look_forward=4)\n",
    "%time accuracy_score(Y_test, estimator.fit(X_train[:, feature_set], Y_train).predict(X_test[:, feature_set]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
